# 2021/5/11
|←|→|
|:---|---:|
go to the [previous page](./10th.md) | go to the [next page](./12th)

## Univ.
### courses
- **Computer Architecture**
    - **NUMA** (Non Uniform Memory Access Model)
    - NCC-NUMA (Non Cache COherent NUMA)
    - CC-NUMA (Cache COherent NUMA)
    - Directory-based cache coherence
    - Memory Consistency Model
        - sequential consistency
        - weak consistency
        - release consistency
    - openMP
- **Computer Architecture**
    - **MPI** (Message Passing Interface)
    - cluster system
    - message passing
    - blocking communication or non-blocking communication
        - non-blocking one is faster than another
    - MPI
        - use many processes to run one program
        - call processes' id "Rank"
        - message passing

- **Computer Algorithm**
    - **String-Searching Algorithms**
    - brute force, KMP, BM

- **Communication Theory**
    - **temporal waveforms & spectrum, InterSymbol Interference (ISI) & Nyquist criterion**
    - (review) *f.t.\[Rect\] = T sinc(wT/2)*, where *T* is width of *Rect*
    - Nyquist pulse
    - Nyquist filter & roll off factor
- **Communication Theory**
    - **Mobile communication**
    - free space propagation model
        - use LOS channel
        - e.g. satellite communication
    - MultiPath Fading

### homework
- **Computer Architecture**
    - Question.  
    Parallelize CG algorithm (conjugate gradient algorithm)
    - Answer.  
    use `#pragma omp parallel for` etc..
- **Computer Architecture**
    - Question.  
    Use MPI to distribute the following program & compare the elapsed time for each Number of Processes.
        ```C
        #define N 16 * 1024
        int i, j;
        double x[N] = {...};
        double sum = 0.0;
        for (i = 0; i < N; ++i) {
            for (j = 0; j < N; ++j) {
                sum += (x[i] - x[j]) * (x[i] - x[j]);
            }
        }
        ```
    - Answer.  
        **i**. pass whole of array `x` to the every process  
        **ii**. in each process, calculate distributed task  
        **iii**. aggregate the partial sum from every process but process-0 to `sum` in the process-0  
        ```C
        #define N 16 * 1024
        int i, j, np;
        double x[N] = {...};
        double sum = 0.0;
        double psum;
        MPI_Status status;
        MPI_Init(&argc, &argv);
        MPI_Comm_rank(MPI_COMM_WORLD, &myid);
        MPI_Comm_size(MPI_COMM_WORLD, &np);

        sum=0.0;
        if (myid == 0) {
            // send x to the each process
            for (i = 1; i < np; ++i) 
                MPI_Send(&x, N, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);

            // process the distributed task
            for (i = 0; i < N/np; ++i) {
                for (j = 0; j < N; ++j) {
                    sum += (x[i] - x[j]) * (x[i] - x[j]);
                }
            }

            // aggregate
            for (i = 1; i < np; ++i) {
                MPI_Recv(&psum, 1, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, &status);
                sum += psum;
            }

            // output
            printf("result: %lf\n", sum);
        } else {
            // receive x
            MPI_Recv(&x, N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &status);

            // process the distributed task
            for (i = N/np * myid; i < N/np * (myid+1); ++i) {
                for (j = 0; j < N; ++j) {
                    sum += (x[i] - x[j]) * (x[i] - x[j]);
                }
            }

            // send the partial sum
            MPI_Send(&sum, 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);
        }

        MPI_Finalize();
        ```
        - *O((L^2)/N  + LN)*  
            , where *L* is the size of `x`, *N* is the number of processes.
        - *O((L^2)/N)* : Distributed Processing
        - *O(LN)* : Message passing

## Exercise
- no contents

## Reading papers, articles, books
- no contents

## Else
- no contents

## MEMO
- really hard..
